{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67180566",
   "metadata": {},
   "source": [
    "### facial_recognition needs dlib and cmake\n",
    "\n",
    "#### dlib \n",
    "It's a landmark's facial detector with pre-trained models, the dlib is used to estimate the location of 68 coordinates (x, y) that map the facial points on a person's face\n",
    "dlib is a toolkit for making real world machine learning and data analysis applications in C++. While the library is originally written in C++, it has good, easy to use Python bindings.\n",
    "\n",
    "#### cmake\n",
    "CMake is an open-source, cross-platform family of tools designed to build, test and package software. CMake is used to control the software compilation process using simple platform and compiler independent configuration files, and generate native makefiles and workspaces that can be used in the compiler environment of your choice. The suite of CMake tools were created by Kitware in response to the need for a powerful, cross-platform build environment for open-source projects such as ITK and VTK.\n",
    "\n",
    "#### Face Recognition\n",
    "Recognize and manipulate faces from Python or from the command line with\n",
    "the world’s simplest face recognition library.\n",
    "Built using dlib’s state-of-the-art face recognition built with deep learning. \n",
    "The model has an accuracy of 99.38% on the\n",
    "Labeled Faces in the Wild benchmark.\n",
    "\n",
    "https://pypi.org/project/face-recognition/\n",
    "\n",
    "https://face-recognition.readthedocs.io/en/latest/face_recognition.html\n",
    "\n",
    "https://face-recognition.readthedocs.io/en/latest/readme.html\n",
    "\n",
    "https://pythonprogramming.net/facial-recognition-python/\n",
    "\n",
    "#### RGB\n",
    "https://www.rapidtables.com/web/color/RGB_Color.html\n",
    "\n",
    "#### RGB to Gray\n",
    "https://web.stanford.edu/class/cs101/image-6-grayscale-adva.html#:~:text=The%20RGB%20scale%20is%20calibrated,%2C%20green%2C%20or%20blue%20hue.\n",
    "\n",
    "https://www.geeksforgeeks.org/matlab-rgb-image-to-grayscale-image-conversion/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fdf7867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 # draw rectangle on face and label image\n",
    "import os\n",
    "import face_recognition\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55ba0955",
   "metadata": {},
   "outputs": [],
   "source": [
    "KNOWN_FACES_DIR = \"known_faces\"\n",
    "# UNKNOWN_FACES_DIR = \"unknown_faces\"\n",
    "TOLERANCE = 0.4 # the lower the tolerance the less chance you have to get false positive\n",
    "# the more is the tolerance the more there will be matching and it can be more risk of incorrect matching.\n",
    "# if tolerance is less then matching will be mostly correct and there can be less mtching and more flase negative\n",
    "\n",
    "FRAME_THICKNESS = 3 # size of frame around face. It depends on how big your image is.\n",
    "FONT_THICKNESS = 2\n",
    "MODEL=\"hog\" # or we can use \"cnn\" model of we use gpu because \"ccn\" is slow in cpu\n",
    "\n",
    "video = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "916efa93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of Darshan photos = 6\n",
      "No of Kishore photos = 4\n",
      "No of Sentdex photos = 3\n",
      "['Darshan', 'Darshan', 'Darshan', 'Darshan', 'Darshan', 'Darshan', 'Kishore', 'Kishore', 'Kishore', 'Kishore', 'Sentdex', 'Sentdex', 'Sentdex']\n"
     ]
    }
   ],
   "source": [
    "known_faces = []\n",
    "known_names = []\n",
    "\n",
    "for name in os.listdir(KNOWN_FACES_DIR):\n",
    "    \n",
    "    f=None\n",
    "    # Next we load every file of faces of known person\n",
    "    for filename in os.listdir(f'{KNOWN_FACES_DIR}/{name}'):\n",
    "        if f==None:\n",
    "            print(f\"No of {name} photos = {len(os.listdir(f'{KNOWN_FACES_DIR}/{name}'))}\")\n",
    "            f=1\n",
    "\n",
    "        # Load an image\n",
    "        image = face_recognition.load_image_file(f'{KNOWN_FACES_DIR}/{name}/{filename}')\n",
    "\n",
    "        # Get 128-dimension face encoding\n",
    "        # Always returns a list of found faces, for this purpose we take first face only (assuming one face per image as you can't be twice on one image)\n",
    "        encoding = face_recognition.face_encodings(image)[0]\n",
    "\n",
    "        # Append encodings and name\n",
    "        known_faces.append(encoding)\n",
    "        known_names.append(name)\n",
    "\n",
    "print(known_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfd922f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing unknown faces\n"
     ]
    }
   ],
   "source": [
    "print(\"processing unknown faces\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c631f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Darshan from [False, False, False, False, True, True, False, False, False, False, False, False, False]\n",
      " - Darshan from [False, False, False, False, True, False, False, False, False, False, False, False, False]\n",
      "Face is not recognised\n",
      " - Darshan from [False, False, False, False, True, True, False, False, False, False, False, False, False]\n",
      "Face is not recognised\n",
      "Face is not recognised\n",
      "Face is not recognised\n",
      " - Darshan from [False, False, False, False, True, True, False, False, False, False, False, False, False]\n",
      "Face is not recognised\n",
      "Face is not recognised\n",
      "Face is not recognised\n",
      "Face is not recognised\n",
      "Face is not recognised\n",
      "Face is not recognised\n",
      "Face is not recognised\n",
      "Face is not recognised\n",
      "Face is not recognised\n",
      "Face is not recognised\n",
      "Face is not recognised\n",
      "Face is not recognised\n",
      "Face is not recognised\n",
      "Face is not recognised\n",
      "Face is not recognised\n",
      "Face is not recognised\n",
      "Face is not recognised\n",
      "Face is not recognised\n",
      "Face is not recognised\n",
      "Face is not recognised\n",
      "Face is not recognised\n",
      "Face is not recognised\n",
      "Face is not recognised\n",
      "Face is not recognised\n",
      "Face is not recognised\n",
      "Face is not recognised\n",
      "Face is not recognised\n",
      "Face is not recognised\n",
      "Face is not recognised\n",
      "Face is not recognised\n",
      "Face is not recognised\n",
      "Face is not recognised\n",
      "Face is not recognised\n",
      " - Darshan from [False, False, False, False, True, False, False, False, False, False, False, False, False]\n",
      "Face is not recognised\n",
      " - Darshan from [False, False, False, False, True, False, False, False, False, False, False, False, False]\n",
      " - Darshan from [False, False, False, False, True, True, False, False, False, False, False, False, False]\n",
      " - Darshan from [False, False, False, False, True, True, False, False, False, False, False, False, False]\n",
      "Face is not recognised\n",
      "Face is not recognised\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    \n",
    "    ret, image = video.read()\n",
    "    \n",
    "    locations = face_recognition.face_locations(image, model=MODEL) # A list of tuples of found face locations in css (top, right, bottom, left) order\n",
    "\n",
    "    encodings = face_recognition.face_encodings(image, locations)\n",
    "    \n",
    "    if locations==[]:\n",
    "        cv2.putText(image, \"No Face is detected in this image\", (0 + 10, 0 + 25), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,0,0), FONT_THICKNESS)\n",
    "\n",
    "    for face_encoding, face_location in zip(encodings, locations):\n",
    "\n",
    "        results = face_recognition.compare_faces(known_faces, face_encoding, TOLERANCE)\n",
    "\n",
    "        match = None\n",
    "        if True in results:\n",
    "            match = known_names[results.index(True)]\n",
    "            print(f' - {match} from {results}')\n",
    "\n",
    "            # Each location contains positions in order: top, right, bottom, left\n",
    "            top_left = (face_location[3], face_location[0])\n",
    "            bottom_right = (face_location[1], face_location[2])\n",
    "\n",
    "            color = [0, 255, 0] # BGR\n",
    "\n",
    "            cv2.rectangle(image, top_left, bottom_right, color, FRAME_THICKNESS)\n",
    "\n",
    "            top_left = (face_location[3], face_location[2])\n",
    "            bottom_right = (face_location[1], face_location[2] + 22)\n",
    "\n",
    "            cv2.rectangle(image, top_left, bottom_right, color, cv2.FILLED)\n",
    "\n",
    "            cv2.putText(image, match, (face_location[3] + 10, face_location[2] + 15), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (43,37,9), FONT_THICKNESS)\n",
    "        else:\n",
    "            print(f\"Face is not recognised\")\n",
    "            # Each location contains positions in order: top, right, bottom, left [x-axis = left,right and y-axis=bottom,top]\n",
    "            top_left = (face_location[3], face_location[0])\n",
    "            bottom_right = (face_location[1], face_location[2])\n",
    "\n",
    "            color = [0, 0, 255] # BGR\n",
    "\n",
    "            cv2.rectangle(image, top_left, bottom_right, color, FRAME_THICKNESS)\n",
    "\n",
    "            top_left = (face_location[3], face_location[2])\n",
    "            bottom_right = (face_location[1], face_location[2] + 22)\n",
    "\n",
    "            cv2.rectangle(image, top_left, bottom_right, color, cv2.FILLED)\n",
    "\n",
    "            cv2.putText(image, \"UNKNOWN\", (face_location[3] + 10, face_location[2] + 15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (43,37,9), FONT_THICKNESS)\n",
    "\n",
    "\n",
    "    cv2.imshow('frame', image)\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "#     cv2.waitKey(10000)\n",
    "#     cv2.destroyWindow(filename)\n",
    "\n",
    "video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e823f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38venv",
   "language": "python",
   "name": "py38venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
